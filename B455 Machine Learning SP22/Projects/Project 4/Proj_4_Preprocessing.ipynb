{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proj 4 Preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zGbAMwy1mOk"
      },
      "source": [
        "Colab doesn't have enough RAM to even create a empty 25,000x89527 matrix that would store each word occurance for every datapoint. \n",
        "My solution is to sort the vocab list by their weight and then split that into groups of roughly equal size. The groups or 'buckets' are words with similar weights and the weight assigned to the whole bucket is the mean of its contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nXc_23Z1PvZ",
        "outputId": "72a3ecac-d7b3-4656-d687-69adf8ea7e27"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "words_file = open('/content/drive/MyDrive/B455/Project 4/Data/imdb.vocab')\n",
        "words = words_file.readlines()\n",
        "words_file.close()\n",
        "\n",
        "weights_file = open('/content/drive/MyDrive/B455/Project 4/Data/imdbEr.txt')\n",
        "word_weights = weights_file.readlines()\n",
        "weights_file.close()\n",
        "\n",
        "with_zeros = False\n",
        "\n",
        "#entires come with a lingering \\n from the file\n",
        "words = [item.replace('\\n', '') for item in words]\n",
        "#converts the entries from 'str' to 'float'\n",
        "word_weights = [float(item) for item in word_weights]\n",
        "\n",
        "print(f'Total number of words: {len(words)}')\n",
        "#total number of word buckets to be created\n",
        "num_buckets = 500\n",
        "\n",
        "df = pd.DataFrame({'words': words, 'weights': word_weights})\n",
        "#sorts the words by their weight\n",
        "df = df.sort_values(by=['weights'])\n",
        "\n",
        "#separates vocab by words with non-zero and zero weights\n",
        "neutral_words = df[df['weights'] == 0]\n",
        "weighted_words = df[df['weights'] != 0]\n",
        "\n",
        "#splits the DataFrame into a python list with num_buckets parts of roughly equal size\n",
        "x = np.array_split(weighted_words, num_buckets, axis=0)\n",
        "#constructs a list of tuples containing (pandas Series containing words, mean weight of all words in the pandas Series)\n",
        "vocab_list = [(frame['words'], np.mean(frame['weights'])) for frame in x]\n",
        "\n",
        "#append zero weighted words\n",
        "if with_zeros:\n",
        "  vocab_list.append((neutral_words['words'], np.mean(neutral_words['weights'])))\n",
        "\n",
        "print(f'Number of \\'buckets\\': {num_buckets}\\nAverage words per \\'bucket\\': {len(vocab_list[0][0])}')\n",
        "print(f'Example bucket:\\n{vocab_list[250]}')\n",
        "\n",
        "# import pickle\n",
        "\n",
        "# f = open('/content/drive/MyDrive/B455/Project 4/Data/vocab_list.p', 'wb')\n",
        "# pickle.dump(vocab_list, f)\n",
        "# f.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words: 89527\n",
            "Number of 'buckets': 500\n",
            "Average words per 'bucket': 113\n",
            "Example bucket:\n",
            "(22309         oily\n",
            "11373     meatball\n",
            "34117      tunisia\n",
            "1918          sean\n",
            "35762    pinkerton\n",
            "           ...    \n",
            "53257      colburn\n",
            "45786        dozor\n",
            "38889         berg\n",
            "7410        rebels\n",
            "678         easily\n",
            "Name: words, Length: 112, dtype: object, 0.30500190816735717)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N3k4_az1Y-C",
        "outputId": "50d816e3-8a80-4d3f-81f6-083248db622f"
      },
      "source": [
        "def read_feat_file(feat_list, vocab_list):\n",
        "  yy = np.array([1 if int(line[0:2]) > 6 else 0 for line in feat_list])\n",
        "  X_temp = [item[2:].split() for item in feat_list]\n",
        "\n",
        "  #multiple occurances of the same word are not taken into account\n",
        "  X_temp = [[int(item.split(':')[0]) for item in lst] for lst in X_temp] #list comprehension is magic\n",
        "\n",
        "  xx = np.zeros((len(feat_list), len(vocab_list)))\n",
        "\n",
        "  #takes about an hour to run this part because there's 3 loops going on for 50,000x500 operations plus extra\n",
        "  #there's probably a better way to do this\n",
        "  for i in range(xx.shape[0]):\n",
        "    if i % 1000 == 0:\n",
        "      print(f'Iter {i}/50000')\n",
        "    for j in range(xx.shape[1]):\n",
        "      #note: multiple occurances of the same word are counted as 1 (from before)\n",
        "      #the value of xx[i,j] represents the number of words from bucket 'j' that appear in item 'i' (which can be more than 1)\n",
        "      xx[i,j] = np.sum([1 if item in vocab_list[j][0] else 0 for item in X_temp[i]])\n",
        "\n",
        "  return (xx, yy)\n",
        "\n",
        "\n",
        "train = open('/content/drive/MyDrive/B455/Project 4/Data/train/labeledBow.feat')\n",
        "test = open('/content/drive/MyDrive/B455/Project 4/Data/test/labeledBow.feat')\n",
        "\n",
        "#bundles all 50,000 samples together to process at once\n",
        "lines = train.readlines()\n",
        "lines.extend(test.readlines())\n",
        "\n",
        "train.close()\n",
        "test.close()\n",
        "\n",
        "data = read_feat_file(lines, vocab_list)\n",
        "\n",
        "#storing the preprocessed data for later because this compuation takes a LONG time\n",
        "if with_zeros:\n",
        "  dump_file = open('/content/drive/MyDrive/B455/Project 4/Data/preprocessed_data.p', 'wb')\n",
        "else:\n",
        "  dump_file = open('/content/drive/MyDrive/B455/Project 4/Data/preprocessed_data_no_zeros.p', 'wb')\n",
        "\n",
        "pickle.dump(data, dump_file)\n",
        "dump_file.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 0/50000\n",
            "Iter 1000/50000\n",
            "Iter 2000/50000\n",
            "Iter 3000/50000\n",
            "Iter 4000/50000\n",
            "Iter 5000/50000\n",
            "Iter 6000/50000\n",
            "Iter 7000/50000\n",
            "Iter 8000/50000\n",
            "Iter 9000/50000\n",
            "Iter 10000/50000\n",
            "Iter 11000/50000\n",
            "Iter 12000/50000\n",
            "Iter 13000/50000\n",
            "Iter 14000/50000\n",
            "Iter 15000/50000\n",
            "Iter 16000/50000\n",
            "Iter 17000/50000\n",
            "Iter 18000/50000\n",
            "Iter 19000/50000\n",
            "Iter 20000/50000\n",
            "Iter 21000/50000\n",
            "Iter 22000/50000\n",
            "Iter 23000/50000\n",
            "Iter 24000/50000\n",
            "Iter 25000/50000\n",
            "Iter 26000/50000\n",
            "Iter 27000/50000\n",
            "Iter 28000/50000\n",
            "Iter 29000/50000\n",
            "Iter 30000/50000\n",
            "Iter 31000/50000\n",
            "Iter 32000/50000\n",
            "Iter 33000/50000\n",
            "Iter 34000/50000\n",
            "Iter 35000/50000\n",
            "Iter 36000/50000\n",
            "Iter 37000/50000\n",
            "Iter 38000/50000\n",
            "Iter 39000/50000\n",
            "Iter 40000/50000\n",
            "Iter 41000/50000\n",
            "Iter 42000/50000\n",
            "Iter 43000/50000\n",
            "Iter 44000/50000\n",
            "Iter 45000/50000\n",
            "Iter 46000/50000\n",
            "Iter 47000/50000\n",
            "Iter 48000/50000\n",
            "Iter 49000/50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppqSkSUo1lSd"
      },
      "source": [
        ""
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaUBIY-rzTjg",
        "outputId": "bb30fee8-571c-4558-8544-e5ad9f5cb359"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# uncomment and run to upload winde.data to google colab if it is not included in the .ipynb file\r\n",
        "# from google.colab import files\r\n",
        "# uploaded = files.upload()\r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "  np.random.seed(55)\r\n",
        "  #load data\r\n",
        "  data = np.loadtxt('wine.data', delimiter=',')\r\n",
        "  names = ['Alcohol', 'Malic Acid', 'Ash', 'Alvalinity of ash', 'Magnesium', 'Total phenols', 'Nonfalvinoid phenols', '{Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\r\n",
        "\r\n",
        "  #shuffle data\r\n",
        "  r = list(range(data.shape[0]))\r\n",
        "  np.random.shuffle(r)\r\n",
        "  data = data[r,:]\r\n",
        "\r\n",
        "  inputs = data[:,1:]\r\n",
        "\r\n",
        "  targets = np.zeros((data[:,0].shape[0], 3))\r\n",
        "  for i, item in enumerate(data[:,0].astype(int) - 1):\r\n",
        "    targets[i] = vectorize(3, item)\r\n",
        "\r\n",
        "  #splits data into 4 parts training and 1 part testing\r\n",
        "  cut_point = int(data.shape[0] / 5)\r\n",
        "  training_inputs = inputs[cut_point:,]\r\n",
        "  training_targets = targets[cut_point:,]\r\n",
        "\r\n",
        "  testing_inputs = inputs[0:cut_point,]\r\n",
        "  testing_targets = targets[0:cut_point,]\r\n",
        "\r\n",
        "  #MLP\r\n",
        "  net = mlp(training_inputs, training_targets, testing_inputs, testing_targets, 10, momentum=0.25)\r\n",
        "  net.train(0.0001, 100)\r\n",
        "\r\n",
        "  testing_inputs = np.concatenate((testing_inputs,-np.ones((testing_inputs.shape[0],1))),axis=1)\r\n",
        "  print('Predictions:\\t', net.predict(testing_inputs))\r\n",
        "  print('Actual Classes:\\t', np.argmax(testing_targets, axis=1))\r\n",
        "\r\n",
        "\r\n",
        "def vectorize(n, i):\r\n",
        "  v = np.zeros(n)\r\n",
        "  v[i] = 1\r\n",
        "\r\n",
        "  return v\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class mlp:\r\n",
        "\r\n",
        "\r\n",
        "  def __init__(self, training_inputs, training_targets, testing_inputs, testing_targets, hidden_nodes, beta=1, momentum=0.0):\r\n",
        "    self.training_inputs = np.concatenate((training_inputs,-np.ones((training_inputs.shape[0],1))),axis=1)\r\n",
        "    self.training_targets = training_targets\r\n",
        "    self.testing_inputs = np.concatenate((testing_inputs,-np.ones((testing_inputs.shape[0],1))),axis=1)\r\n",
        "    self.testing_targets = testing_targets\r\n",
        "\r\n",
        "    self.in_count = training_inputs.shape[1]\r\n",
        "    self.out_count = training_targets.shape[1]\r\n",
        "    self.training_data_count = training_inputs.shape[0]\r\n",
        "    self.testing_data_count = testing_inputs.shape[0]\r\n",
        "\r\n",
        "    self.hidden_count = hidden_nodes\r\n",
        "    \r\n",
        "    self.beta = beta\r\n",
        "    self.momentum = momentum\r\n",
        "\r\n",
        "    #weights from input to hidden layer\r\n",
        "    self.weights_IH = (np.random.rand(self.in_count + 1,self.hidden_count) - 0.5) * 2 / np.sqrt(self.in_count)\r\n",
        "\r\n",
        "    #weights from hidden layer to output \r\n",
        "    self.weights_HO = (np.random.rand(self.hidden_count + 1,self.out_count) - 0.5) * 2 / np.sqrt(self.hidden_count)\r\n",
        "\r\n",
        "\r\n",
        "  def train(self, eta, iterations):\r\n",
        "    weights_IH_delta = np.zeros((np.shape(self.weights_IH)))\r\n",
        "    weights_HO_delta = np.zeros((np.shape(self.weights_HO)))\r\n",
        "\r\n",
        "    for epoch in range(iterations):\r\n",
        "      print('Epoch #', epoch)\r\n",
        "\r\n",
        "      #shuffle data\r\n",
        "      self.shuffle_data()\r\n",
        "\r\n",
        "      #Learning algorithm\r\n",
        "      for i in range(self.training_inputs.shape[0]):\r\n",
        "        row = np.reshape((self.training_inputs[i,]), (1, self.in_count + 1))\r\n",
        "        outputs = self.forwardpass(row)\r\n",
        "\r\n",
        "        #calculate sigmas\r\n",
        "        sigma_out = (outputs - self.training_targets[i,]) * outputs * (1 - outputs)\r\n",
        "        sigma_hidden = self.beta * self.hidden * (1 - self.hidden) * np.dot(sigma_out, self.weights_HO.transpose())\r\n",
        "        #remove the biases\r\n",
        "        sigma_hidden = sigma_hidden[:,:-1]\r\n",
        "\r\n",
        "        #Calcute weight deltas\r\n",
        "        weights_HO_delta = eta * (np.dot(self.hidden.transpose(), sigma_out)) + (self.momentum * weights_HO_delta)\r\n",
        "        weights_IH_delta = eta * (np.dot(row.transpose(), sigma_hidden)) + (self.momentum * weights_IH_delta)\r\n",
        "\r\n",
        "        #update weights\r\n",
        "        self.weights_HO -= weights_HO_delta\r\n",
        "        self.weights_IH -= weights_IH_delta\r\n",
        "      #Accuracy Calculation\r\n",
        "      predictions = self.predict(self.testing_inputs)\r\n",
        "      actual = np.argmax(self.testing_targets, axis=1)\r\n",
        "      accuracy = sum(predictions == actual) / len(predictions) * 100\r\n",
        "      print(f'\\tAccuracy: {accuracy:0.2f}%')\r\n",
        "      \r\n",
        "\r\n",
        "  def forwardpass(self, inputs):\r\n",
        "    self.hidden = np.dot(inputs,self.weights_IH)\r\n",
        "    self.hidden = self.sigmoid(-self.beta * self.hidden)\r\n",
        "    self.hidden = np.concatenate((self.hidden,-np.ones((inputs.shape[0],1))),axis=1)\r\n",
        "\r\n",
        "    outputs = np.dot(self.hidden,self.weights_HO)\r\n",
        "\r\n",
        "    return self.softmax(outputs)\r\n",
        "\r\n",
        "\r\n",
        "  def predict(self, input):\r\n",
        "    return np.argmax(self.forwardpass(input), axis=1)\r\n",
        "\r\n",
        "\r\n",
        "  def shuffle_data(self):\r\n",
        "    training_shuffle = list(range(self.training_data_count))\r\n",
        "    testing_shuffle = list(range(self.testing_data_count))\r\n",
        "\r\n",
        "    np.random.shuffle(training_shuffle)\r\n",
        "    self.training_inputs = self.training_inputs[training_shuffle,:]\r\n",
        "    self.training_targets = self.training_targets[training_shuffle,:]\r\n",
        "      \r\n",
        "    np.random.shuffle(testing_shuffle)\r\n",
        "    self.testing_inputs = self.testing_inputs[testing_shuffle,:]\r\n",
        "    self.testing_targets = self.testing_targets[testing_shuffle,:]\r\n",
        "\r\n",
        "\r\n",
        "  def sigmoid(self, x):\r\n",
        "    return 1.0 / (1.0 + np.exp(-x))\r\n",
        "\r\n",
        "\r\n",
        "  def softmax(self, v):\r\n",
        "    for row in range(v.shape[0]):\r\n",
        "      a = np.exp(v[row])\r\n",
        "      v[row] = a / np.sum(a)\r\n",
        "     \r\n",
        "    return v\r\n",
        "\r\n",
        "  def print_data(self):\r\n",
        "    print('Input Nodes:', self.in_count, '\\nHidden Nodes:', self.hidden_count, '\\nOutput Nodes:', self.out_count, '\\n# Data points:', self.training_data_count)\r\n",
        "    print('Weights IH shape:', self.weights_IH.shape, '\\nWeights HO shape: ', self.weights_HO.shape)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "  main()"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch # 0\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 1\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 2\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 3\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 4\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 5\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 6\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 7\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 8\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 9\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 10\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 11\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 12\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 13\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 14\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 15\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 16\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 17\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 18\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 19\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 20\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 21\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 22\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 23\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 24\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 25\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 26\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 27\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 28\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 29\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 30\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 31\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 32\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 33\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 34\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 35\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 36\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 37\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 38\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 39\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 40\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 41\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 42\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 43\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 44\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 45\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 46\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 47\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 48\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 49\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 50\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 51\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 52\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 53\n",
            "\tAccuracy: 34.29%\n",
            "Epoch # 54\n",
            "\tAccuracy: 22.86%\n",
            "Epoch # 55\n",
            "\tAccuracy: 22.86%\n",
            "Epoch # 56\n",
            "\tAccuracy: 22.86%\n",
            "Epoch # 57\n",
            "\tAccuracy: 22.86%\n",
            "Epoch # 58\n",
            "\tAccuracy: 22.86%\n",
            "Epoch # 59\n",
            "\tAccuracy: 22.86%\n",
            "Epoch # 60\n",
            "\tAccuracy: 22.86%\n",
            "Epoch # 61\n",
            "\tAccuracy: 22.86%\n",
            "Epoch # 62\n",
            "\tAccuracy: 22.86%\n",
            "Epoch # 63\n",
            "\tAccuracy: 22.86%\n",
            "Epoch # 64\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 65\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 66\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 67\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 68\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 69\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 70\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 71\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 72\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 73\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 74\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 75\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 76\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 77\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 78\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 79\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 80\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 81\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 82\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 83\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 84\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 85\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 86\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 87\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 88\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 89\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 90\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 91\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 92\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 93\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 94\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 95\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 96\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 97\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 98\n",
            "\tAccuracy: 42.86%\n",
            "Epoch # 99\n",
            "\tAccuracy: 42.86%\n",
            "Predictions:\t [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Actual Classes:\t [0 2 1 2 2 1 0 1 0 2 2 2 1 0 1 2 0 0 2 0 1 2 2 1 1 1 1 2 1 1 0 1 1 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTeQcFeuODZQ"
      },
      "source": [
        "Report: It doesn't work. The model begins by predicting random classes (expected) but converges to predicting every input as one class. Changing beta, momentum, or hidden nodes, or number of epochs doesn't keep it from predicting just one class for everything. Right now, I dont know where the error(s) I've made are. As I understand it (which could be wrong) the backprop algorithm is working as it should which means I'm giving it bad data but I don't think I am."
      ]
    }
  ]
}
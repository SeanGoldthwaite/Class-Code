
# B457 Assignment 1 - Group 9

## How to Run

Our program is run from the command line and has a few arguments.

### Mandatory Arguments

```
Input:      path to the png file for the program to analyze
Display:    -d to automatically display the final annotated image on completion
```

Example:  ```py omr.py ../test-images/music1.png -d```
to analyze music1.png and show the result when the program in done.

### Optional Arguments

These are all False by default and options in the 'Restrictions' category are mutually exclusive
```
Outputs: 
	-o to save results to different location (default is to an 'outputs' folder)
	--no-out to not save results to a file

Restrictions:	
	--only-detect-staffs to perform detection only on the staves
	--only-detect-notes to perform detection only on the notes
	--only-detect-notes to perform detection only on the rests
``` 

## Design Decisions

### Assumptions

All the note names are calculated assuming the top staff is treble and the bottom is bass for a pair of staffs. Theoretically you could solve this by detecting the clef symbols in the same way as the notes and rests are detected. Notes would the be named according the to first clef symbol encountered on the same staff when looking to the left.

Key Signatures (and accidentals) don't exist. Our group discussed implementing this is we had extra time, but we ended up focusing on the main assignment. Using a template for a sharp or flat would yield a number of matches directly after the clef symbol. Determining the size of such a cluster would be enough to know which notes are sharp/flat. Accidentals would be easy to find with a template but potentially difficult to assign to a specific note, especially if the notes are in a chord.

The music sheet is assumed to be vertically aligned, and staffs are assumed to take up at least ten pixels vertically. We also assume that all staffs are of the same height, and that the larger a staff is, the less likely it is. The orientation of a sheet of music could likely be determined by finding the mode of orientations generated by the sobel operator, but it is not necessary for this data set.

#### Staff Detection

The general process for detecting staffs is as follows:

* First, we need to find the edges in the image.
	* To do this, we first apply the sobel operator to the given image to get edge magnitude and orientation maps.
	* Using the magnitude and orientation maps, we create a version of the magnitude map with non-maximals supressed.
	* Then, we apply canny thresholding on the magnitude map to ensure that only edges are being picked up.
* Now, we need to create a Hough space map with parameters `center` and `radius` representing possible positions of staffs in the image. The `center` parameter refers to the position of the center line in the staff, and `radius` refers to half of the distance between the top bar and bottom bar in the staff.
	* For every row in the edge magnitude map, we take the sum of the entire row divided by the width of the image to normalize it. Since some staffs may be shorter than others, we take the square root of this value to 'boost' it up a bit.
	* now, we use this calculated value to cast a 'vote' in the Hough space map for every staff that could have generated it.
		* If this row were generated by a staff such that it were the center line, then its y value in Hough space is the same as its actual y value. So, we draw a horizontal line in Hough space at its y value.
		* If this row were generated by a staff such that it were above or below the horizontal line, then the staff's radius will change linearly depending on the staff's center in relation to the row's y position. So, we draw a diagonal line out from `(center=row.y, size=0)` outwards, with a slope depending on the staff line this row is on (i.e. `m` is one of `{-1, -0.5, 0.5, 1}`).
		* We need to mark out all radius values less than 10, since we assumed earlier that it takes ten pixels to make a staff. We also need to darken pixels in the Hough space map as they approach higher radii, since we also assumed that larger staffs are less likely.
	* Finally, we normalize the Hough space map depending on its max value.
* We now need to find out the radii of all of the staffs. All we need to do is find the point in the Hough space map with the highest value. We now know the scale of the staffs in the image.
* We now take a 1D slice of the Hough space map for the given radii, and apply a 1D gaussian blur to this slice depending on the radius we found to help smooth out some of the values for later. This slice represents all possible staffs of the radius we found earlier.
* Now, we supress non-maximals in the slice. I wrote a special non-maximal supression function for 1D space that takes into account a 'dip threshold', basically allowing for maximal values to supress non-maximal values even if there are slight peaks.
* Finally, we are left with points in the slice that represent the center positions of the staffs in the image, and all we need to do is extract them. For this, we also set a minimum value for a point to be extracted.

### Note Detection

Our note and rest detection both rely on using the hamming distance implemented as a convolution. For the notes specifically, since they are spherical, a template that is slightly "off center" also returns a match. This produces "mini notes" which should be reduced to one match per note with a clustering algorithm. 

Fortunately we learned a number of clustering algorithms that don't rely on knowing the number of clusters beforehand. Mean-shift clustering is theoretically perfect for this task since the cluster size is known but in the name of efficiency we implemented a modified version that takes ideas from BFS. The very general idea for the algorithm is as follows

```
Instantiate a list of clusters
set max_distance as the distance of the diagonal of the note template
Loop over the pixels
	When a match is found, call it (i, j)
	Start a new cluster with that point
	loop over all points in the cluster:
		Generate neighbors of point that are also matches and less than max_distance to (i, j)
		If there are more than 3 points, add all of them to new cluster
		If the lenght of the current cluster exceeds 75, go to the next point
		Set all points are not detected (so they aren't added twice)
	
	If the cluster is bigger than a threshold
		Calculate the center of the new cluster and add it to the list of clusters
```

Even with these methods our program still detects a decent number of false notes. These mainly come from very thick eighth and especially sixteenth note bars that are a similar height to the notes themselves. A good example of this is the sixteenth notes in music2.png whose bars are solid like a note would be.

Theoretically these false positives are eliminated by eliminating clusters that are 'too long' and 'too thin', but in practice quantifying those values was tricky without affecting the detection of notes in chords where the clusters may have blended together (specifically chords with 3rds and 2nds).

Our main solution was to stop adding new points to a cluster when they are further away from the starting point than the max distance in the template. This splits the chord-blended notes into two as desired but ends up splitting the false positives into even more FPs.  

### Rest Detection

For rests, we use the same hamming distance method as for notes. Unlike the note template, the rest templates have very irregular shapes and they also include the staff lines in the template. This means that an off center rest template is not likely to produce a match in the same way an off center note template would. On the other hand, a small difference in the scale of the staff compared the template would cause a rest to be completely missed. In music3.png there are also some rests that are fully below the staff with no staff lines through them which results in them being missed by the template.

![music3](report_images\music3.png "music3 analysis")

Our rest detection parameters work well for the first image, but fail on the other images. In the future we would use a more sophisticated approach for the high-detail templates like rests.

![music1](report_images\music1.png "music1 analysis")

One suggestion we considered was to use intensity maps in our hamming scoring function, to weight the more distinctive features of the rest templates higher than the blank space around the rest in our scoring function.
